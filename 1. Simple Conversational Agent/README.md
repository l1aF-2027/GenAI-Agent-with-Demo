
# 1. Simple Conversational Agent

<img width="1862" height="957" alt="image" src="https://github.com/user-attachments/assets/cc7d7d78-d351-4e45-8c83-fba1b2767eb3" />

## ğŸ“– Overview

This project demonstrates how to build a **context-aware conversational agent** using the **Gemini 2.5 Flash** language model.
Unlike many chatbots that forget past messages, this agent maintains **chat history in memory** to provide coherent and natural multi-turn conversations.

ğŸ’¡ *Note: No chat history is persisted to a database (e.g., Prisma/Neon); memory is reset on backend restart.*


## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py           # FastAPI backend with WebSocket endpoint for real-time chat
â”‚   â”œâ”€â”€ llm.py           # LLM wrapper + in-memory chat history management
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile       # Containerization support
â”‚   â””â”€â”€ test.py          # Backend tests
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ app/             # Next.js app (pages, layouts, API routes)
    â”œâ”€â”€ components/      # React components (chat UI, auth, etc.)
    â”œâ”€â”€ hooks/           # Custom React hooks (WebSocket, toasts)
    â”œâ”€â”€ lib/             # Utility functions (auth, websocket client)
    â”œâ”€â”€ prisma/          # Prisma schema (âš ï¸ not used for chat history)
    â”œâ”€â”€ public/          # Static assets
    â”œâ”€â”€ scripts/         # Database scripts (âš ï¸ not used for chat history)
    â””â”€â”€ ...              # Config files & build artifacts
```



## âœ¨ Key Features

* **ğŸ¤– Language Model:** Gemini 2.5 Flash for conversational AI.
* **ğŸ§  Context Awareness:** Maintains chat history per session (in memory).
* **âš¡ Real-Time:** WebSocket communication with FastAPI backend.
* **ğŸ’» Modern Frontend:** Next.js + React chat interface.
* **ğŸ“¦ Lightweight:** No database integration for chat logs.


## âš™ï¸ How It Works

1. **Session Management** â†’ Each chat session has a unique `session_id`.
2. **In-Memory Chat History** â†’ User and AI messages stored in `chat_history[session_id]`.
3. **Prompt Construction** â†’ System instructions + history + latest user input.
4. **AI Response** â†’ Gemini 2.5 Flash generates context-aware replies.
5. **Frontend** â†’ Next.js UI communicates with backend via WebSocket.


## ğŸ¯ Motivation

Most chatbots fail to maintain **context**, leading to broken or repetitive interactions.
This project improves **conversation flow** by keeping session history, making responses feel more natural.

## ğŸ”§ Extensibility

* ğŸ”„ **Switch LLMs:** Replace Gemini with another model.
* ğŸ’¾ **Persistent Storage:** Connect Prisma/Neon for saving chats.
* ğŸ¨ **Custom Prompts:** Adapt for customer support, sales, education, etc.

## âš ï¸ Limitations

* **Ephemeral History** â†’ Lost if backend restarts.
* **No Chat DB** â†’ Prisma/Neon only for auth & user data (not conversations).

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install Dependencies

```bash
# Backend
pip install -r requirements.txt

# Frontend
npm install
```

### 2ï¸âƒ£ Set Up API Keys & Config

Create a `.env` file in **backend** and **frontend**.

```bash
# Backend
GOOGLE_API_KEY=xxx   # https://ai.google.dev/gemini-api/docs

# Frontend
DATABASE_URL=xxx     # https://console.neon.tech/
NEXTAUTH_SECRET=xxx  # https://generate-secret.vercel.app/32
NEXTAUTH_URL="http://localhost:3000"

# Google OAuth (from Google Cloud Console)
GOOGLE_CLIENT_ID=xxx
GOOGLE_CLIENT_SECRET=xxx

# WebSocket API
NEXT_PUBLIC_FASTAPI_URL="ws://127.0.0.1:8000"
```

### 3ï¸âƒ£ Run the Applications

```bash
# Backend (FastAPI)
uvicorn app:app --host 0.0.0.0 --port 8000

# Frontend (Next.js)
npm run dev
```

### 4ï¸âƒ£ Start Chatting ğŸ‰

Open `http://localhost:3000` in your browser and chat with the AI agent!

## ğŸ“Œ Notes

This repo is a **foundation** for building more advanced conversational systems.
You can extend it with:

* Persistent chat history
* RAG (Retrieval-Augmented Generation)
* Sentiment analysis
* Task automation
